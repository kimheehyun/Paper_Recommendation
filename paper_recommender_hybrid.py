import streamlit as st
import arxiv
import requests
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import pandas as pd
from datetime import datetime
from groq import Groq
import os
import time
import math

# ========================================
# Groq API ì„¤ì •
# ========================================
groq_api_key = st.secrets.get("GROQ_API_KEY", os.getenv("GROQ_API_KEY"))
client = Groq(api_key=groq_api_key)

# ========================================
# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸° ë° í˜ì´ì§€ ì„¤ì •
# ========================================
st.set_page_config(page_title="ë…¼ë¬¸ ì¶”ì²œ ì±—ë´‡", layout="wide")
st.title("ë…¼ë¬¸ ì¶”ì²œ ì±—ë´‡")
st.write("arXiv + Semantic Scholar Co-Citationì„ í™œìš©í•œ í•˜ì´ë¸Œë¦¬ë“œ ë…¼ë¬¸ ì¶”ì²œ ì„œë¹„ìŠ¤")

@st.cache_resource
def load_model():
    return SentenceTransformer("all-MiniLM-L6-v2", device="cpu")

model = load_model()

# ========================================
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ========================================
if "messages" not in st.session_state:
    st.session_state.messages = []
if "papers_cache" not in st.session_state:
    st.session_state.papers_cache = {}
if "citations_cache" not in st.session_state:
    st.session_state.citations_cache = {}
if "last_papers" not in st.session_state:
    st.session_state.last_papers = None
if "last_scores" not in st.session_state:
    st.session_state.last_scores = None
if "last_semantic_sim" not in st.session_state:
    st.session_state.last_semantic_sim = None
if "last_citations" not in st.session_state:
    st.session_state.last_citations = None
if "last_recency" not in st.session_state:
    st.session_state.last_recency = None
if "last_co_citation" not in st.session_state:
    st.session_state.last_co_citation = None
if "last_explanation" not in st.session_state:
    st.session_state.last_explanation = None

# ========================================
# arXiv ë…¼ë¬¸ ê°€ì ¸ì˜¤ê¸°
# ========================================
def fetch_arxiv_papers(query, max_results=50):
    """max_resultsë¥¼ 50ìœ¼ë¡œ ì¦ê°€ (2ë‹¨ê³„ í•„í„°ë§ì„ ìœ„í•´)"""
    try:
        client_arxiv = arxiv.Client()
        search = arxiv.Search(
            query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance
        )
        papers = list(client_arxiv.results(search))
        df = pd.DataFrame(
            {
                "title": [p.title for p in papers],
                "abstract": [p.summary if p.summary else "" for p in papers],
                "arxiv_id": [p.entry_id.split("/abs/")[-1] for p in papers],
                "doi": [p.doi if p.doi else "" for p in papers],
                "published": [p.published.strftime("%Y-%m-%d") for p in papers],
                "authors": [
                    ", ".join([author.name for author in p.authors[:3]]) for p in papers
                ],
            }
        )
        return df
    except Exception as e:
        st.error(f"arXiv ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        return pd.DataFrame()

# ========================================
# Semantic Scholar ì •ë³´ ê°€ì ¸ì˜¤ê¸°
# ========================================
def fetch_semanticscholar_info(title, arxiv_id):
    cache_key = title + "_" + arxiv_id
    if cache_key in st.session_state.papers_cache:
        return st.session_state.papers_cache[cache_key]

    default_result = {
        "paper_id": "",
        "citation_count": 0,
        "influential_citation_count": 0,
        "publication_date": "",
        "found_by": "not_found"
    }

    def search_ss(query_type, query_value):
        try:
            if query_type == "title":
                url = "https://api.semanticscholar.org/graph/v1/paper/search"
                params = {
                    "query": query_value,
                    "limit": 1,
                    "fields": "paperId,citationCount,influentialCitationCount,publicationDate,title"
                }
                res = requests.get(url, params=params, timeout=5)
                if res.status_code == 200:
                    data = res.json()
                    if data.get("data") and len(data["data"]) > 0:
                        paper = data["data"][0]
                        return paper, "title"

            elif query_type == "arxiv_id":
                paper_id = f"ARXIV:{query_value}"
                url_id = f"https://api.semanticscholar.org/graph/v1/paper/{paper_id}"
                params_id = {
                    "fields": "paperId,citationCount,influentialCitationCount,publicationDate,title"
                }
                res = requests.get(url_id, params=params_id, timeout=5)
                if res.status_code == 200:
                    paper = res.json()
                    if paper.get("paperId"): 
                        return paper, "arxiv_id"
        except Exception:
            return None, None
        return None, None

    paper_info, found_by = search_ss("title", title)
    
    if not paper_info and arxiv_id:
        paper_info, found_by = search_ss("arxiv_id", arxiv_id)

    if paper_info:
        result = {
            "paper_id": paper_info.get("paperId", ""),
            "citation_count": paper_info.get("citationCount", 0),
            "influential_citation_count": paper_info.get("influentialCitationCount", 0),
            "publication_date": paper_info.get("publicationDate", ""),
            "found_by": found_by
        }
        st.session_state.papers_cache[cache_key] = result
        time.sleep(0.15)
        return result
    
    st.session_state.papers_cache[cache_key] = default_result
    return default_result

# ========================================
# ê°œì„ ëœ ì¸ìš© ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìºì‹± + ì¬ì‹œë„)
# ========================================
def get_citing_papers(paper_id, max_retries=2):
    """
    íŠ¹ì • ë…¼ë¬¸ì„ ì¸ìš©í•œ ë…¼ë¬¸ì˜ ID ì§‘í•© ë°˜í™˜ (ìºì‹± ì ìš©)
    """
    if not paper_id:
        return set()
    
    # ìºì‹œ í™•ì¸
    if paper_id in st.session_state.citations_cache:
        return st.session_state.citations_cache[paper_id]
    
    for attempt in range(max_retries):
        try:
            url = f"https://api.semanticscholar.org/graph/v1/paper/{paper_id}"
            params = {
                "fields": "citations.paperId",
            }
            res = requests.get(url, params=params, timeout=8)
            
            if res.status_code == 200:
                data = res.json()
                citing_papers = {
                    c["paperId"] for c in data.get("citations", []) 
                    if c.get("paperId")
                }
                
                # ìºì‹œì— ì €ì¥
                st.session_state.citations_cache[paper_id] = citing_papers
                time.sleep(0.2)
                return citing_papers
                
            elif res.status_code == 429:
                wait_time = (attempt + 1) * 2
                time.sleep(wait_time)
                continue
                
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(1)
                continue
    
    # ì‹¤íŒ¨ ì‹œ ë¹ˆ ì§‘í•© ìºì‹±
    st.session_state.citations_cache[paper_id] = set()
    return set()

# ========================================
# ë‘ ë…¼ë¬¸ ê°„ ê³µë™ì¸ìš© ì ìˆ˜ ê³„ì‚°
# ========================================
def cocite_score(a_id, b_id):
    """
    ë‘ ë…¼ë¬¸ ê°„ ê³µë™ ì¸ìš© ì ìˆ˜ (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ë°©ì‹)
    """
    A = get_citing_papers(a_id)
    B = get_citing_papers(b_id)
    
    if not A or not B:
        return 0.0
    
    n_common = len(A & B)
    return n_common / math.sqrt(len(A) * len(B))

# ========================================
# Seed-based ê³µë™ì¸ìš© ì ìˆ˜ ê³„ì‚° (ê°œì„  ë²„ì „)
# ========================================
def seed_based_cocitation_score(candidate_id, seed_ids):
    """
    ì‹œë“œ ë…¼ë¬¸ ì§‘í•© ê¸°ë°˜ ê³µë™ ì¸ìš© ì ìˆ˜
    ê° ì‹œë“œ ë…¼ë¬¸ê³¼ì˜ ê³µë™ì¸ìš© ì ìˆ˜ë¥¼ í‰ê· ë‚´ì–´ ë°˜í™˜
    """
    if not candidate_id or not seed_ids:
        return 0.0
    
    scores = []
    for seed_id in seed_ids:
        if candidate_id == seed_id:  # ìê¸° ìì‹ ì€ ì œì™¸
            continue
        score = cocite_score(candidate_id, seed_id)
        if score > 0:
            scores.append(score)
    
    return sum(scores) / len(scores) if scores else 0.0

# ========================================
# ê³µë™ì¸ìš© ì ìˆ˜ ê³„ì‚° (Seed-based ë°©ì‹)
# ========================================
def build_seed_based_co_citation_scores(paper_ids, seed_window=5):
    """
    ê°œì„ ëœ Seed-based ê³µë™ì¸ìš© ì ìˆ˜ ê³„ì‚°
    - ìƒìœ„ Nê°œë¥¼ ì‹œë“œë¡œ ì„ ì •
    - ê° í›„ë³´ ë…¼ë¬¸ê³¼ ì‹œë“œë“¤ ê°„ì˜ ê³µë™ì¸ìš© ì ìˆ˜ë¥¼ ê³„ì‚°
    """
    if not paper_ids:
        return np.zeros(len(paper_ids))

    # 1ë‹¨ê³„: ì‹œë“œ ë…¼ë¬¸ ì„ ì •
    valid_seed_ids = [pid for pid in paper_ids[:seed_window] if pid]
    
    if not valid_seed_ids:
        st.warning("âš ï¸ ìœ íš¨í•œ ì‹œë“œ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ê³µë™ì¸ìš© ë¶„ì„ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return np.zeros(len(paper_ids))
    
    st.info(f"ğŸŒ± ìƒìœ„ {len(valid_seed_ids)}ê°œ ë…¼ë¬¸ì„ ì‹œë“œë¡œ ì„ ì •")
    
    # 2ë‹¨ê³„: ì‹œë“œ ë…¼ë¬¸ë“¤ì˜ ì¸ìš© ì •ë³´ ìˆ˜ì§‘ (ìºì‹±ë¨)
    st.info(f"ğŸ” {len(valid_seed_ids)}ê°œ ì‹œë“œ ë…¼ë¬¸ì˜ ì¸ìš© ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
    for idx, seed_id in enumerate(valid_seed_ids):
        citing_count = len(get_citing_papers(seed_id))
        st.caption(f"   âœ“ ì‹œë“œ {idx+1}: {citing_count}ê°œ ì¸ìš© ë°œê²¬")
    
    # 3ë‹¨ê³„: ëª¨ë“  í›„ë³´ ë…¼ë¬¸ì˜ ê³µë™ì¸ìš© ì ìˆ˜ ê³„ì‚°
    st.info(f"âš™ï¸ {len(paper_ids)}ê°œ í›„ë³´ ë…¼ë¬¸ì˜ ê³µë™ì¸ìš© ì ìˆ˜ ê³„ì‚° ì¤‘...")
    scores = []
    
    for idx, candidate_id in enumerate(paper_ids):
        if not candidate_id:
            scores.append(0.0)
            continue
        
        score = seed_based_cocitation_score(candidate_id, valid_seed_ids)
        scores.append(score)
        
        # ì§„í–‰ìƒí™© í‘œì‹œ
        if (idx + 1) % 5 == 0:
            st.caption(f"   ì²˜ë¦¬ ì¤‘: {idx+1}/{len(paper_ids)}")
    
    # 4ë‹¨ê³„: ì •ê·œí™”
    scores = np.array(scores)
    max_score = scores.max()
    
    if max_score > 0:
        scores = scores / max_score
        st.success(f"âœ“ Seed-based ê³µë™ì¸ìš© ë¶„ì„ ì™„ë£Œ! (ìµœëŒ€ ì ìˆ˜: {max_score:.4f})")
    else:
        st.warning("âš ï¸ ìœ ì˜ë¯¸í•œ ê³µë™ì¸ìš© íŒ¨í„´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    return scores

# ========================================
# ì¶”ì²œ ì ìˆ˜ ê³„ì‚° (2ë‹¨ê³„ í•„í„°ë§ ë°©ì‹)
# ========================================
def calculate_recommendation_score(papers_df, query_embedding, top_n=10, use_two_stage=True):
    """
    use_two_stage=True: 50ê°œ ìˆ˜ì§‘ â†’ ì¸ìš© ê¸°ë°˜ í•„í„°ë§ â†’ 15ê°œë¡œ ì••ì¶• â†’ ì •ë°€ ë¶„ì„
    use_two_stage=False: ê¸°ì¡´ ë°©ì‹ (30ê°œ ëª¨ë‘ ë¶„ì„)
    """
    papers_df = papers_df.copy()
    papers_df["abstract"] = papers_df["abstract"].fillna("").astype(str)
    
    # ì„ë² ë”© ê³„ì‚°
    texts = papers_df["title"].astype(str) + ". " + papers_df["abstract"].astype(str)
    embeddings = model.encode(texts.tolist())
    
    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„
    semantic_scores = cosine_similarity([query_embedding], embeddings)[0]
    
    # ============================================================
    # 1ë‹¨ê³„: ë¹ ë¥¸ í•„í„°ë§ (ì¸ìš©ìˆ˜ + ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ë¡œ í›„ë³´ ì••ì¶•)
    # ============================================================
    if use_two_stage and len(papers_df) > 15:
        st.info("ğŸ” 1ë‹¨ê³„: ì¸ìš©ìˆ˜ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§ ì¤‘...")
        
        # Semantic Scholar ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë¹ ë¥¸ í•„í„°ë§ìš©)
        quick_citation_scores = []
        for idx, row in papers_df.iterrows():
            info = fetch_semanticscholar_info(title=row["title"], arxiv_id=row["arxiv_id"])
            citation_count = info["citation_count"]
            quick_citation_scores.append(citation_count)
        
        # ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„ + ì¸ìš©ìˆ˜ë¡œ 1ì°¨ ì ìˆ˜ ê³„ì‚°
        quick_citation_scores = np.array(quick_citation_scores)
        normalized_citations = quick_citation_scores / (quick_citation_scores.max() + 1)
        
        normalized_semantic = semantic_scores / (semantic_scores.max() + 0.001)
        
        # 1ì°¨ ì ìˆ˜: ì˜ë¯¸(70%) + ì¸ìš©(30%)
        quick_scores = 0.7 * normalized_semantic + 0.3 * normalized_citations
        
        # ìƒìœ„ 15ê°œë§Œ ì„ íƒ (ì •ë°€ ë¶„ì„ ëŒ€ìƒ)
        top_15_idx = np.argsort(quick_scores)[::-1][:15]
        papers_df = papers_df.iloc[top_15_idx].reset_index(drop=True)
        semantic_scores = semantic_scores[top_15_idx]
        embeddings = embeddings[top_15_idx]
        
        st.success(f"âœ“ ìƒìœ„ 15ê°œ í›„ë³´ë¡œ ì••ì¶• ì™„ë£Œ (ì¸ìš©ìˆ˜ ë²”ìœ„: {quick_citation_scores[top_15_idx].min():.0f}~{quick_citation_scores[top_15_idx].max():.0f}íšŒ)")
    
    # ============================================================
    # 2ë‹¨ê³„: ì •ë°€ ë¶„ì„ (ê³µë™ì¸ìš© í¬í•¨)
    # ============================================================
    st.info("ğŸ” 2ë‹¨ê³„: ì •ë°€ ë¶„ì„ ì‹œì‘...")
    
    # Semantic Scholar ì •ë³´ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸° (ìºì‹œ í™œìš©)
    citation_scores = []
    recency_scores = []
    ss_info_list = []
    paper_ids = []
    
    for idx, row in papers_df.iterrows():
        info = fetch_semanticscholar_info(title=row["title"], arxiv_id=row["arxiv_id"]) 
        ss_info_list.append(info)
        paper_ids.append(info["paper_id"])
        
        citation_count = info["citation_count"]
        citation_score = min(citation_count / 100, 1.0) if citation_count > 0 else 0
        citation_scores.append(citation_score)
        
        # ìµœì‹ ì„± ì ìˆ˜
        pub_date = datetime.strptime(row["published"], "%Y-%m-%d")
        days_old = (datetime.now() - pub_date).days
        recency_score = max(1 - (days_old / 3650), 0)
        recency_scores.append(recency_score)
    
    # Seed-based ê³µë™ì¸ìš© ì ìˆ˜ ê³„ì‚°
    st.divider()
    co_citation_scores = build_seed_based_co_citation_scores(
        paper_ids, seed_window=5
    )
    
    # ìµœì¢… ì ìˆ˜ ê³„ì‚°
    citation_scores = np.array(citation_scores)
    recency_scores = np.array(recency_scores)
    co_citation_scores = np.array(co_citation_scores)
    
    # semantic_scores ì •ê·œí™”
    if semantic_scores.max() > 0:
        normalized_semantic = semantic_scores / semantic_scores.max()
    else:
        normalized_semantic = semantic_scores
    
    # ìµœì¢… ê°€ì¤‘ì¹˜: ì˜ë¯¸(50%) + ì¸ìš©(20%) + ìµœì‹ ì„±(10%) + ê³µë™ì¸ìš©(20%)
    final_scores = (
        0.50 * normalized_semantic
        + 0.20 * citation_scores
        + 0.10 * recency_scores
        + 0.20 * co_citation_scores
    )
    
    top_idx = np.argsort(final_scores)[::-1][:top_n]
    result_df = papers_df.iloc[top_idx].reset_index(drop=True)
    result_scores = final_scores[top_idx]
    semantic_sim = semantic_scores[top_idx]
    citations = [citation_scores[i] for i in top_idx]
    recency = [recency_scores[i] for i in top_idx]
    co_citation = [co_citation_scores[i] for i in top_idx]
    
    # ê²°ê³¼ì— ì¶”ê°€ ì •ë³´ í¬í•¨
    result_df["citation_count"] = [ss_info_list[i]["citation_count"] for i in top_idx]
    result_df["found_by"] = [ss_info_list[i]["found_by"] for i in top_idx]
    result_df["co_citation_score"] = co_citation
    
    return result_df, result_scores, semantic_sim, citations, recency, co_citation

# ========================================
# LLM ì„¤ëª… ìƒì„±
# ========================================
def generate_recommendation_explanation(user_query, recommended_papers):
    papers_info = ""
    for idx, row in recommended_papers.iterrows():
        co_citation_info = f"\nCo-citation Score: {row.get('co_citation_score', 0):.3f}" if row.get('co_citation_score', 0) > 0 else ""
        
        papers_info += f"\n---\nPaper {idx+1}: {row['title']}\nAuthors: {row['authors']}\nPublished: {row['published']}\nCitations: {row.get('citation_count', 0)}{co_citation_info}\nAbstract: {row['abstract'][:300]}...\n"
    
    prompt = f"""The user is interested in the following field: "{user_query}"

Analyze the abstracts of the recommended papers below. The 'Co-citation Score' reflects how frequently the candidate is cited together with the seed papers across the literature. Provide a concise abstract summary and a professional explanation of why the paper was recommended.

The output MUST be in Korean and strictly follow the format below. Separate the analysis of each paper using the exact phrase: ###END_OF_PAPER_ANALYSIS###

{papers_info}

Format:
- ì´ˆë¡ ìš”ì•½ [N]: [ê°„ë‹¨í•œ ì„¤ëª…] \n
- ë…¼ë¬¸ ì¶”ì²œ ê·¼ê±° [N]: [ê°„ë‹¨í•œ ì„¤ëª…]
###END_OF_PAPER_ANALYSIS###
"""
    
    try:
        message = client.chat.completions.create(
            model="llama-3.3-70b-versatile",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1024,
            temperature=0.7,
        )
        return message.choices[0].message.content
    except Exception as e:
        return f"LLM ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {str(e)}"

# ========================================
# ì±—ë´‡ ì…ë ¥ ì²˜ë¦¬
# ========================================
def chat_with_user(user_input):
    with st.spinner("ì§€ê¸ˆ arXivì—ì„œ ê´€ë ¨ ë…¼ë¬¸ì„ ê²€ìƒ‰í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        papers_df = fetch_arxiv_papers(user_input, max_results=50)
        
    if papers_df.empty:
        response = "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì£¼ì œì˜ ë…¼ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ì‹œë„í•´ ì£¼ì„¸ìš”."
        st.session_state.messages.append({"role": "assistant", "content": response})
        st.rerun() 
        return
        
    query_embedding = model.encode(user_input)
    
    with st.spinner("ì§€ê¸ˆ Semantic Scholarì—ì„œ ì¸ìš© ì •ë³´ ë° Seed-based ê³µë™ì¸ìš© ë¶„ì„ ì¤‘..."):
        rec_papers, scores, semantic_sim, citations, recency, co_citation = (
            calculate_recommendation_score(papers_df, query_embedding, top_n=5)
        )
        
    with st.spinner("ì§€ê¸ˆ LLMì´ ì¶”ì²œ ì´ìœ ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
        explanation = generate_recommendation_explanation(user_input, rec_papers)
        
    response = f"**'{user_input}'** ê´€ë ¨ ì¶”ì²œ ë…¼ë¬¸ {len(rec_papers)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤! ì•„ë˜ì—ì„œ ìƒì„¸ ì •ë³´ì™€ LLM ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ ì£¼ì„¸ìš”."
    
    st.session_state.messages.append({"role": "assistant", "content": response})
    
    st.session_state.last_papers = rec_papers
    st.session_state.last_scores = scores
    st.session_state.last_semantic_sim = semantic_sim
    st.session_state.last_citations = citations
    st.session_state.last_recency = recency
    st.session_state.last_co_citation = co_citation
    st.session_state.last_explanation = explanation
    
    st.rerun()

# ========================================
# UI ë ˆì´ì•„ì›ƒ
# ========================================

message_count = len(st.session_state.messages)
if message_count > 0:
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

user_query = st.chat_input("ê´€ì‹¬ ìˆëŠ” ë¶„ì•¼ë‚˜ ë…¼ë¬¸ ì£¼ì œë¥¼ ì…ë ¥í•˜ì„¸ìš”(ì˜ì–´ë¡œ ì…ë ¥í•˜ëŠ” ê²ƒì´ ê²€ìƒ‰ ì •í™•ë„ì— ìœ ë¦¬í•©ë‹ˆë‹¤).")

if user_query:
    st.session_state.messages.append({"role": "user", "content": user_query})
    chat_with_user(user_query)

# ìµœì‹  ì¶”ì²œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´
if st.session_state.last_papers is not None and not st.session_state.last_papers.empty:
    rec_papers = st.session_state.last_papers
    scores = st.session_state.last_scores
    semantic_sim = st.session_state.last_semantic_sim
    citations = st.session_state.last_citations
    recency = st.session_state.last_recency
    co_citation = st.session_state.last_co_citation
    
    st.divider()
    st.subheader("ìµœì‹  ì¶”ì²œ ë…¼ë¬¸ ìƒì„¸ ì •ë³´")
    
    for idx, row in rec_papers.iterrows():
        with st.expander(f"**{idx+1}. {row['title']}**"):
            col1, col2 = st.columns([2, 1])
            with col1:
                st.write(f"**ì €ì:** {row['authors']}")
                st.write(f"**ë°œí‘œì¼:** {row['published']}")
                st.write(f"**arXiv ID:** {row['arxiv_id']}")
                if row.get('doi'):
                    st.write(f"**DOI:** {row['doi']}")
                st.write(f"**ì¸ìš©ìˆ˜:** {row.get('citation_count', 0)}íšŒ")
                st.write(f"**ê²€ìƒ‰ë°©ë²•:** {row.get('found_by', 'N/A')}")
                st.write(f"\n**ì´ˆë¡:**\n{row['abstract']}")
            with col2:
                st.metric("ì¶”ì²œ ì ìˆ˜", f"{scores[idx]:.3f}")
                st.metric("ì˜ë¯¸ë¡ ì  ìœ ì‚¬ë„", f"{semantic_sim[idx]:.3f}")
                st.metric("ì¸ìš© ê¸°ë°˜ ì ìˆ˜", f"{citations[idx]:.3f}")
                st.metric("ìµœì‹ ì„± ì ìˆ˜", f"{recency[idx]:.3f}")
                st.metric("Seed-based ê³µë™ì¸ìš©", f"{co_citation[idx]:.3f}",
                        help="ìƒìœ„ ì‹œë“œ ë…¼ë¬¸ë“¤ê³¼ì˜ í‰ê·  ê³µë™ì¸ìš© ì ìˆ˜ì…ë‹ˆë‹¤. ì‹œë“œì™€ í•¨ê»˜ ì¸ìš©ë˜ëŠ” ë¹ˆë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.")
            
            paper_url = f"https://arxiv.org/abs/{row['arxiv_id']}"
            st.markdown(f"[arXivì—ì„œ ë³´ê¸°]({paper_url})")

# LLM ë¶„ì„ ê²°ê³¼
if st.session_state.last_explanation:
    st.divider()
    st.subheader("ìµœì‹  LLM ë…¼ë¬¸ ì´ˆë¡ ìš”ì•½ ë° ì¶”ì²œ ë¶„ì„")
    
    analysis_parts = st.session_state.last_explanation.split("###END_OF_PAPER_ANALYSIS###")
    
    for i, part in enumerate(analysis_parts):
        cleaned_part = part.strip()
        if cleaned_part:
            if i > 0:
                st.divider()
            st.markdown(cleaned_part)
